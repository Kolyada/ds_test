{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# импортируем библиотеки для визуализации\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline\n\n# Загружаем специальный удобный инструмент для разделения датасета:\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-18T00:18:44.320473Z","iopub.execute_input":"2022-03-18T00:18:44.321006Z","iopub.status.idle":"2022-03-18T00:18:45.615575Z","shell.execute_reply.started":"2022-03-18T00:18:44.320881Z","shell.execute_reply":"2022-03-18T00:18:45.614493Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\nRANDOM_SEED = 42","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:45.617219Z","iopub.execute_input":"2022-03-18T00:18:45.617495Z","iopub.status.idle":"2022-03-18T00:18:45.621838Z","shell.execute_reply.started":"2022-03-18T00:18:45.617464Z","shell.execute_reply":"2022-03-18T00:18:45.620905Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n!pip freeze > requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:45.622988Z","iopub.execute_input":"2022-03-18T00:18:45.623192Z","iopub.status.idle":"2022-03-18T00:18:50.304948Z","shell.execute_reply.started":"2022-03-18T00:18:45.623167Z","shell.execute_reply":"2022-03-18T00:18:50.303924Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Подгрузим наши данные из соревнования\n\nDATA_DIR = '/kaggle/input/sf-booking/'\ndf_train = pd.read_csv(DATA_DIR+'/hotels_train.csv') # датасет для обучения\ndf_test = pd.read_csv(DATA_DIR+'hotels_test.csv') # датасет для предсказания\nsample_submission = pd.read_csv(DATA_DIR+'/submission.csv') # самбмишн","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:50.307675Z","iopub.execute_input":"2022-03-18T00:18:50.307950Z","iopub.status.idle":"2022-03-18T00:18:57.186455Z","shell.execute_reply.started":"2022-03-18T00:18:50.307918Z","shell.execute_reply":"2022-03-18T00:18:57.185653Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.188034Z","iopub.execute_input":"2022-03-18T00:18:57.188421Z","iopub.status.idle":"2022-03-18T00:18:57.560978Z","shell.execute_reply.started":"2022-03-18T00:18:57.188377Z","shell.execute_reply":"2022-03-18T00:18:57.560009Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.562279Z","iopub.execute_input":"2022-03-18T00:18:57.563338Z","iopub.status.idle":"2022-03-18T00:18:57.588492Z","shell.execute_reply.started":"2022-03-18T00:18:57.563302Z","shell.execute_reply":"2022-03-18T00:18:57.587623Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.589900Z","iopub.execute_input":"2022-03-18T00:18:57.590487Z","iopub.status.idle":"2022-03-18T00:18:57.721850Z","shell.execute_reply.started":"2022-03-18T00:18:57.590429Z","shell.execute_reply":"2022-03-18T00:18:57.721194Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_test.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.722994Z","iopub.execute_input":"2022-03-18T00:18:57.723218Z","iopub.status.idle":"2022-03-18T00:18:57.741347Z","shell.execute_reply.started":"2022-03-18T00:18:57.723188Z","shell.execute_reply":"2022-03-18T00:18:57.740498Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"sample_submission.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.742906Z","iopub.execute_input":"2022-03-18T00:18:57.743987Z","iopub.status.idle":"2022-03-18T00:18:57.760016Z","shell.execute_reply.started":"2022-03-18T00:18:57.743936Z","shell.execute_reply":"2022-03-18T00:18:57.759323Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sample_submission.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.765630Z","iopub.execute_input":"2022-03-18T00:18:57.766345Z","iopub.status.idle":"2022-03-18T00:18:57.777335Z","shell.execute_reply.started":"2022-03-18T00:18:57.766308Z","shell.execute_reply":"2022-03-18T00:18:57.776632Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\ndf_train['sample'] = 1 # помечаем где у нас трейн\ndf_test['sample'] = 0 # помечаем где у нас тест\ndf_test['reviewer_score'] = 0 # в тесте у нас нет значения reviewer_score, мы его должны предсказать, по этому пока просто заполняем нулями\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:57.778752Z","iopub.execute_input":"2022-03-18T00:18:57.779198Z","iopub.status.idle":"2022-03-18T00:18:58.515195Z","shell.execute_reply.started":"2022-03-18T00:18:57.779165Z","shell.execute_reply":"2022-03-18T00:18:58.514203Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.corr(),annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:58.516458Z","iopub.execute_input":"2022-03-18T00:18:58.516692Z","iopub.status.idle":"2022-03-18T00:18:59.659936Z","shell.execute_reply.started":"2022-03-18T00:18:58.516662Z","shell.execute_reply":"2022-03-18T00:18:59.659123Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#    nltk используем для оценки текста комментария\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.downloader.download('vader_lexicon')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:18:59.661490Z","iopub.execute_input":"2022-03-18T00:18:59.662013Z","iopub.status.idle":"2022-03-18T00:19:20.395352Z","shell.execute_reply.started":"2022-03-18T00:18:59.661966Z","shell.execute_reply":"2022-03-18T00:19:20.394234Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"%%time\n\n#    Используем Counter для подсчета популярных тегов средствами Python, а не Pandas\n#    LabelEncoder используем для передачи номинальных категориальных признаков в модель напрямую\n\nfrom collections import Counter\nfrom collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder\n\n\n#Поскольку отель встречается несколько раз, делаем из него номинальный признак\ndata['hotel_name'] = LabelEncoder().fit_transform(data['hotel_name'])\n\n#Отели расположены в 5 городах, используем это\ndata['hotel_location'] = data['hotel_address'].apply(lambda s: ' '.join((s.split(' ')[-2:])))\ndata.drop('hotel_address',axis = 1, inplace = True)\n\n#    Преобразуем координаты следующим образом:\n#    Расчитаем минимальное, максимальное и среднее значение широты и долготы, для каждого города.\n#    Заменим пропуски на среднее значение\n#    Выполним операцию MinMaxScaler, с учетом каждого города \n#    вычтем из координаты отеля минимальную широту и долготу соответственно для его города\n#    разделим на разницу между максимальным и минимальным значение координаты\n#    Значения широта и долгота примут значения от 0 до 1 для каждого города\n\naverage_coords = dict(data.groupby('hotel_location')[['lat','lng']].agg(['min','mean','max']))\ndef align_coord_to_center(row,key,avg):\n    value = row[key]\n    if np.isnan(row[key]):\n        value = avg[(key,'mean')][row['hotel_location']]\n    result = (value - avg[(key,'min')][row['hotel_location']]) / (avg[(key,'max')][row['hotel_location']] - avg[(key,'min')][row['hotel_location']])\n    return result\n\ndata['lat'] = data.apply(align_coord_to_center,axis = 1,key = 'lat',avg = average_coords)\ndata['lng'] = data.apply(align_coord_to_center,axis = 1,key = 'lng',avg = average_coords)\n\n#Преобразуем город отеля в бинарные признаки методом OneHotEncoding, удалим исходный признак\ndata = pd.concat([data,pd.get_dummies(data['hotel_location'], drop_first = True)],axis=1)\ndata.drop('hotel_location',axis = 1, inplace = True)\n\n#Оставим признак - возраст комментария в числовом виде для передачи в модель\ndata['days_since_review'] = data['days_since_review'].apply(lambda s: s.split(' ')[0]).astype('int')\n\n#Из даты оставит также месяц и день недели\ndata['review_date'] = pd.to_datetime(data['review_date'])\ndata['month'] = pd.to_datetime(data['review_date']).dt.month\ndata['weekday'] = pd.to_datetime(data['review_date']).dt.weekday\ndata.drop('review_date',axis = 1, inplace = True)\n\n#Из множества национальностей автора отзыва оставим наиболее популярные, остальные соберем в группу Other и закодируем методов OneHotEncoding\nallowed_nationalities = ['United Kingdom', 'United States of America', 'Australia', 'Ireland']\ndata = pd.concat([data,pd.get_dummies(data['reviewer_nationality'].str.strip().apply(lambda s: s if s in allowed_nationalities else 'Other'),prefix = 'From ',drop_first=True)], axis = 1)\ndata.drop('reviewer_nationality',axis = 1, inplace = True)\n\n\n#    Поработаем с текстом комментариев:\n#    Видимо при заполнении отзыва предлагалось два поля: \"Хорошее\" и \"Плохое\"\n#    И люди иногда писали \"ничего\" в одно из полей\n#    Поскольку мы будем полагаться исключительно на размер комментария, нужно избавиться от таких случаев\n#    Наиболее частые представлены в словаре, затрём их для обоих полей\n#    Затем пересчитаем поля \"количество слов в плохих/хороших комментариях\"\n#    Также создадим новый признак - разница между размеров комментариев\n#    Дополнительно оценим полярность комментариев с помощью пакета nltk\n#    Просуммируем результаты оценки обоих текстов\n\nreplace = {    \n    'positive_review': ['no positive','nothing'],\n    'negative_review': [\n        'nothing really','no negative','nothing','n a','none','nothing at all','nothing to dislike',\n        'everything was perfect','na','can t think of anything','nil','everything was great','absolutely nothing',\n        'nothing to complain about','no','nothing not to like','nothing all good','no complaints','i liked everything'\n        ,'liked everything'\n    ]\n    \n}\n\nfor key in replace:\n    for value in replace[key]:\n        data.loc[data[key].str.strip().str.lower()==value,key] = ''\n        \ndata['review_total_positive_word_counts'] = data['positive_review'].apply(lambda s: len(s.strip().split(' ')))\ndata['review_total_negative_word_counts'] = data['negative_review'].apply(lambda s: len(s.strip().split(' ')))\n\ndata['review_diff'] = data['positive_review'].str.len() - data['negative_review'].str.len()\n\npolarity_columns = ['neg','neu','pos','compound']\nanalyzer = SentimentIntensityAnalyzer()\n\ndef get_polarity(row, analyzer):\n    positive_counter = Counter(analyzer.polarity_scores(row['positive_review']))\n    negative_counter = Counter(analyzer.polarity_scores(row['negative_review']))\n    result_counter = positive_counter + negative_counter\n    return [result_counter[col] for col in polarity_columns]\n\npolarities = list(data.apply(get_polarity, analyzer = analyzer, axis = 1))\n\ndata_polarity = pd.DataFrame(polarities,columns = ['neg','neu','pos','compound'])\n\ndata = pd.concat([data, data_polarity], axis = 1)\n\ndata.drop(['positive_review','negative_review'],axis = 1, inplace = True)\n\n\n#Из всего набора тегов оставим 10 наиболее популярных в виде бинарных признаков\nTAGS_NUMBER = 11\nc = Counter()\nfor tags in data['tags'].apply(lambda s: [x.strip() for x in s[1:-1].replace(\"'\",'').split(',')]):\n    for tag in tags:\n        c[tag] += 1\nfor tag , _ in c.most_common(TAGS_NUMBER):\n    tag_name = tag.lower().replace(' ','_')\n    data[f\"tag_{tag_name}\"] = data['tags'].apply(lambda tags: int(tag in tags))        \ndata.drop(['tags','tag_leisure_trip'],axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:19:20.397297Z","iopub.execute_input":"2022-03-18T00:19:20.397616Z","iopub.status.idle":"2022-03-18T00:25:47.112838Z","shell.execute_reply.started":"2022-03-18T00:19:20.397570Z","shell.execute_reply":"2022-03-18T00:25:47.111948Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#    Для сравнения степени влияния признаков на целевую переменную воспользуемся тестами\n#    Хи-квадрат для категориальных переменных\n#    ANOVA для числовых переменных\n#    Дальнейшее удаление признаков приводит к ухудшению итогового значения MAPE\n\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import f_classif\n\ncat_cols = [\n    'Barcelona Spain','Milan Italy','Paris France','United Kingdom','Vienna Austria','month','weekday',\n    'From _Ireland','From _Other','From _United Kingdom','From _United States of America',\n    'tag_submitted_from_a_mobile_device','tag_couple','tag_stayed_1_night','tag_stayed_2_nights','tag_solo_traveler',\n    'tag_stayed_3_nights','tag_business_trip','tag_group','tag_family_with_young_children','tag_stayed_4_nights'\n]\nnum_cols = [\n    'additional_number_of_scoring','average_score','review_total_negative_word_counts','total_number_of_reviews',\n    'review_total_positive_word_counts','total_number_of_reviews_reviewer_has_given','days_since_review','lat','lng',\n    'review_diff'\n]","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:47.114244Z","iopub.execute_input":"2022-03-18T00:25:47.115081Z","iopub.status.idle":"2022-03-18T00:25:47.190133Z","shell.execute_reply.started":"2022-03-18T00:25:47.115034Z","shell.execute_reply":"2022-03-18T00:25:47.189357Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"pd.Series(chi2(data[cat_cols],data['reviewer_score'].astype('int'))[0],index=cat_cols).sort_values(ascending=True).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:47.191544Z","iopub.execute_input":"2022-03-18T00:25:47.191873Z","iopub.status.idle":"2022-03-18T00:25:48.482101Z","shell.execute_reply.started":"2022-03-18T00:25:47.191843Z","shell.execute_reply":"2022-03-18T00:25:48.481178Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pd.Series(f_classif(data[num_cols],data['reviewer_score'].astype('int'))[0],index=num_cols).sort_values(ascending=True).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:48.483657Z","iopub.execute_input":"2022-03-18T00:25:48.484610Z","iopub.status.idle":"2022-03-18T00:25:48.884398Z","shell.execute_reply.started":"2022-03-18T00:25:48.484571Z","shell.execute_reply":"2022-03-18T00:25:48.883771Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"data.nunique(dropna=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:48.885575Z","iopub.execute_input":"2022-03-18T00:25:48.886220Z","iopub.status.idle":"2022-03-18T00:25:49.048758Z","shell.execute_reply.started":"2022-03-18T00:25:48.886183Z","shell.execute_reply":"2022-03-18T00:25:49.047932Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15,10)\nsns.heatmap(data.drop(['sample'], axis=1).corr(), annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:49.050208Z","iopub.execute_input":"2022-03-18T00:25:49.051079Z","iopub.status.idle":"2022-03-18T00:25:58.169355Z","shell.execute_reply.started":"2022-03-18T00:25:49.051032Z","shell.execute_reply":"2022-03-18T00:25:58.168382Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.170769Z","iopub.execute_input":"2022-03-18T00:25:58.171045Z","iopub.status.idle":"2022-03-18T00:25:58.178542Z","shell.execute_reply.started":"2022-03-18T00:25:58.171008Z","shell.execute_reply":"2022-03-18T00:25:58.177828Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# убираем признаки которые еще не успели обработать, \n# модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\nobject_columns = [s for s in data.columns if data[s].dtypes == 'object']\ndata.drop(object_columns, axis = 1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.179775Z","iopub.execute_input":"2022-03-18T00:25:58.180171Z","iopub.status.idle":"2022-03-18T00:25:58.224658Z","shell.execute_reply.started":"2022-03-18T00:25:58.180123Z","shell.execute_reply":"2022-03-18T00:25:58.223840Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.226075Z","iopub.execute_input":"2022-03-18T00:25:58.226523Z","iopub.status.idle":"2022-03-18T00:25:58.274041Z","shell.execute_reply.started":"2022-03-18T00:25:58.226491Z","shell.execute_reply":"2022-03-18T00:25:58.273436Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Теперь выделим тестовую часть\ntrain_data = data.query('sample == 1').drop(['sample'], axis=1)\ntest_data = data.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.reviewer_score.values            # наш таргет\nX = train_data.drop(['reviewer_score'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.275093Z","iopub.execute_input":"2022-03-18T00:25:58.275657Z","iopub.status.idle":"2022-03-18T00:25:58.416067Z","shell.execute_reply.started":"2022-03-18T00:25:58.275619Z","shell.execute_reply":"2022-03-18T00:25:58.415000Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n# выделим 20% данных на валидацию (параметр test_size)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.417644Z","iopub.execute_input":"2022-03-18T00:25:58.417977Z","iopub.status.idle":"2022-03-18T00:25:58.566788Z","shell.execute_reply.started":"2022-03-18T00:25:58.417933Z","shell.execute_reply":"2022-03-18T00:25:58.566099Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# проверяем\ntest_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.567729Z","iopub.execute_input":"2022-03-18T00:25:58.568437Z","iopub.status.idle":"2022-03-18T00:25:58.574973Z","shell.execute_reply.started":"2022-03-18T00:25:58.568400Z","shell.execute_reply":"2022-03-18T00:25:58.573933Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Импортируем необходимые библиотеки:\nfrom sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\nfrom sklearn import metrics # инструменты для оценки точности модели","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.576158Z","iopub.execute_input":"2022-03-18T00:25:58.576425Z","iopub.status.idle":"2022-03-18T00:25:58.636154Z","shell.execute_reply.started":"2022-03-18T00:25:58.576384Z","shell.execute_reply":"2022-03-18T00:25:58.635447Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\nmodel = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n#model = RandomForestRegressor(n_estimators=20, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.637572Z","iopub.execute_input":"2022-03-18T00:25:58.637790Z","iopub.status.idle":"2022-03-18T00:25:58.642960Z","shell.execute_reply.started":"2022-03-18T00:25:58.637764Z","shell.execute_reply":"2022-03-18T00:25:58.642334Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Обучаем модель на тестовом наборе данных\nmodel.fit(X_train, y_train)\n\n# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n# Предсказанные значения записываем в переменную y_pred\ny_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:25:58.646754Z","iopub.execute_input":"2022-03-18T00:25:58.647208Z","iopub.status.idle":"2022-03-18T00:29:38.438578Z","shell.execute_reply.started":"2022-03-18T00:25:58.647176Z","shell.execute_reply":"2022-03-18T00:29:38.437587Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\nprint('MAPE:', metrics.mean_absolute_error(y_test, y_pred))\n#MAPE: 0.8785584855418102 добавлен nltk - compound\n#MAPE: 0.8741241323147318 nltk - все параметры","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:38.439889Z","iopub.execute_input":"2022-03-18T00:29:38.440207Z","iopub.status.idle":"2022-03-18T00:29:38.448724Z","shell.execute_reply.started":"2022-03-18T00:29:38.440164Z","shell.execute_reply":"2022-03-18T00:29:38.447850Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\nplt.rcParams['figure.figsize'] = (10,10)\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:38.450089Z","iopub.execute_input":"2022-03-18T00:29:38.450337Z","iopub.status.idle":"2022-03-18T00:29:38.970911Z","shell.execute_reply.started":"2022-03-18T00:29:38.450308Z","shell.execute_reply":"2022-03-18T00:29:38.970304Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_data.sample(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:38.971880Z","iopub.execute_input":"2022-03-18T00:29:38.973596Z","iopub.status.idle":"2022-03-18T00:29:39.004665Z","shell.execute_reply.started":"2022-03-18T00:29:38.973431Z","shell.execute_reply":"2022-03-18T00:29:39.003357Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"test_data = test_data.drop(['reviewer_score'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:39.006285Z","iopub.execute_input":"2022-03-18T00:29:39.006600Z","iopub.status.idle":"2022-03-18T00:29:39.024311Z","shell.execute_reply.started":"2022-03-18T00:29:39.006557Z","shell.execute_reply":"2022-03-18T00:29:39.023129Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:39.025516Z","iopub.execute_input":"2022-03-18T00:29:39.025746Z","iopub.status.idle":"2022-03-18T00:29:39.036791Z","shell.execute_reply.started":"2022-03-18T00:29:39.025720Z","shell.execute_reply":"2022-03-18T00:29:39.035867Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"predict_submission = model.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:39.038685Z","iopub.execute_input":"2022-03-18T00:29:39.039039Z","iopub.status.idle":"2022-03-18T00:29:41.982346Z","shell.execute_reply.started":"2022-03-18T00:29:39.038994Z","shell.execute_reply":"2022-03-18T00:29:41.981679Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"predict_submission","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:41.983208Z","iopub.execute_input":"2022-03-18T00:29:41.983449Z","iopub.status.idle":"2022-03-18T00:29:41.990008Z","shell.execute_reply.started":"2022-03-18T00:29:41.983419Z","shell.execute_reply":"2022-03-18T00:29:41.989074Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"list(sample_submission)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:41.991813Z","iopub.execute_input":"2022-03-18T00:29:41.992489Z","iopub.status.idle":"2022-03-18T00:29:42.003704Z","shell.execute_reply.started":"2022-03-18T00:29:41.992443Z","shell.execute_reply":"2022-03-18T00:29:42.002826Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sample_submission['reviewer_score'] = predict_submission\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T00:29:42.004888Z","iopub.execute_input":"2022-03-18T00:29:42.005121Z","iopub.status.idle":"2022-03-18T00:29:42.429885Z","shell.execute_reply.started":"2022-03-18T00:29:42.005085Z","shell.execute_reply":"2022-03-18T00:29:42.429313Z"},"trusted":true},"execution_count":37,"outputs":[]}]}